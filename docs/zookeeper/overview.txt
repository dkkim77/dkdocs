###########################################
			Raft Consensus Algorithm 
###########################################			
1. 구성 요소
- 리더 : 클러스터를 대표하는 노드. 클라이언트가 클러스터로 보낸 모든 명령의 수신 및 전파, 응답을 전담.
           자신의 heartbeat 를 주기적으로 모든 팔로워에게 전파.
- 팔로워 : 리더로부터 수신된 명령을 처리
- 후보자 : 리더가 없을 때 리더의 heartbeat 를 못받게 되고 팔로워들은 이 상태가 된다.
             	
2. 처리 절차
- 리더는 수신된 명령에 대해 로그를 생성하여 로컬에 저장한 후 팔로워들에게 복제. 팔로워들은 수신 후 리더에 응답
- 리더가 팔로워로부터 수신한 정상 응답 수가 전체 과반 수에 이르면  클러스터의 모든 노드가 로그를 통해 전파된 명령을 수행하도록 함.
  (과반 수가 안되면 클러스터는 서비스 불가 상태) 
  클러스터 전체 노드가 동일하게 보유할 때까지 주기적으로 로그를 재전송한다.
 - 장애때문에 명령을 처리하지 못한 팔로워가 있더라도 복구된 뒤 그 동안의 로그들을 재수신받아 순차적으로 수행.
 
3. 리더 선출
  3.1 용어 
  - term : 투표 회차 (임기)
  - election timeout : 후보자로 전환하기까지의 대기 시간. 
  
  3.2 절차
  - 리더가 없는 상태에서 모든 노드는 election timeout 이 될 때까지 팔로워 상태를 유지한다.
  - election timeout 이 먼저 끝난 노드부터 후보자로 전환되고 해당 후보자는 즉시 자신에게 한표를 준 뒤 다른 노드들에게 투표 요청을 보낸다.
  - 투표 요청을 받은 노드가 해당 term 에 투표한 적이 없다면 요청을 보낸 후보자에게 투표하고 election timeout 을 초기화한다.
     이는 더 이상 후보자가 출현하지 않도록 하는 장치이다.
  - 전체 노드의 과반 수를 얻은 후보자가 리더로 선정된다.
  
4. 리더가 장애가 생긴 후 복구될 때...
  - election timeout 이내에 리더로부터 메시지를 받지 못하면 팔로워는 즉시 후보자가 된다. 이 때 클러스터의 term 이 증가하게 되고 
     새로운 리더가 선출된다.
  - 각 노드들은 term 을 저장하고 메시지 송수신 시 term 을 포함시킨다. 장애가 생겼던 이전 리더가 복구되어 클러스터에 참여하면 자신의 term 이
     더 낮은 것을 알게되고 팔로워로 전환된다.
     
5. 과반을 얻지 못할 경우
  - 4개 노드에서 우연히 2개 후보자가 각각 2표씩 얻었을 때 term 을 증가시키고 재선거를 실시한다. 
     알고리즘은 각 노드들의 election timeout 을 random 하게 재조정한다. 
      
###########################################
			ZOOKEEPER
###########################################			
			
1. 분산 시스템 설계 이슈 
- 분산된 시스템간의 정보를 어떻게 공유할 것
- 클러스터에 있는 서버들의 상태를 체크
- 분산된 서버들간에 동기화를 위한 락(lock)을 처리

2. 코디네이션 서비스 
- 분산 시스템 내에서 중요한 상태 정보나 설정 정보등을 유지
- 이중화등을 통하여 고가용성을 제공
- fail over/fail back 가능

3. ZooKeeper 활용 시나리오

- 큐 : Watcher와 Sequence node를 이용하면, 큐를 구현할 수 있는데, 
			Queue 라는 Node를 만든 후에, 이 노드의 Child node를 sequence node로 구성하면, 
			새롭게 생성되는 메세지들은 이 sequence node로 순차적으로 생성된다. 
			이 큐를 읽는 클라이언트가 이 큐 node를 watch 하도록 설정하면, 이 클라이언트는 새로운 메세지가 들어올 때마다 
			call back을 받아서, 마치 메세지 Queue의 pub/sub 과 같은 형태의 효과를 낼 수 있다. 
			대용량 메세지나 애플리케이션 성격상의 메세지는 일반적인 큐 솔루션인 Rabbit MQ등을 활용하고, 
			ZooKeeper는 클러스터간 통신용 큐로 활용하는 것을 고려해볼 수 있다.
			
- 서버 설정 정보 : 가장 일반적인 용도로는 클러스터 내의 각 서버들의 설정 정보(Configuration)를 저장하는 저장소로 쓸 수 있다. 
			정보가 안정적으로 저장이 될 뿐 아니라. Watch 기능을 이용하면, 설정 정보가 저장될 경우, 각 서버로 알려서 바로 반영을 할 수 있다.

- 클러스터 정보 : 현재 클러스터에서 기동중인 서버 목록을 유지할 수 있다. 
			Ephemeral Node는 Zookeeper 클라이언트가 살아 있을 경우에만 유효하기 때문에, 
			클러스터내의 각 서버가 Ephemeral Node를 등록하도록 하면, 해당 서버가 죽으면 
			Ephemeral Node 가 삭제 되기 때문에 클러스터 내의 살아 있는 Node 리스트만 유지할 수 있다. 
			조금 더 발전하면, 클러스터가 master/slave 구조일때, master 서버가 죽었을 때, 
			다른 master 서버를 선출하는 election logic 도 구현이 가능하다. 
			(https://zookeeper.apache.org/doc/r3.4.6/recipes.html#sc_recipes_Queues 참고)


- 글로벌 락 : 여러개의 서버로 구성된 분산 서버가 공유 자원을 접근하려고 했을때, 
					동시에 하나의 작업만이 발생해야 한다고 할때, 그 작업에 Lock을 걸고 작업을 할 수 있는 기능을 구현할때 사용한다. 
					
----------------------------------------------------------------------------------------------------
CAP 정리 : 아래의 조건을 모두 충족하는 분산 시스템은 없다
- Consistency(일관성): 모든 노드가 같은 데이터를 볼 수 있다.
- Availability(가용성): 모든 요청이 성공, 실패 여부를 반환할 수 있다.
- Partition-tolerance(분할 내성): 메시지 전달이 실패하거나 시스템 일부에 문제가 발생하더라도 전체 시스템이 원활하게 동작할 수 있다
* 일관성과 가용성 중 하나를 선택하라는 의미로 해석. 분산시스템 설계 시 어느 정도의 일관성과 가용성이 필요한지 정의가 선행되야 함.
* 예시 : 만일 두 노드가 서로 통신할 수 없다면 일부 혹은 모든 요청에 응답할 수 없게 되거나(일관성 높음, 가용성 낮음) 
        아니면 요청을 처리하게 됨으로써 각 노드에서 데이터의 불일치가 발생할 수 있다.					
					